{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2_2021389_DNN\n",
    "## Task_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19242267],\n",
       "       [0.75256253]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = [0.2,0.3]\n",
    "# weights = [[0.1,0.2],[0.4,0.5]]\n",
    "# bias = [0.6,0.4]\n",
    "\n",
    "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
    "    weights_hidden = np.random.rand(input_dim, hidden_dim)\n",
    "    bias_hidden = np.random.rand(1, hidden_dim)\n",
    "    weights_output = np.random.rand(hidden_dim, output_dim)\n",
    "    bias_output = np.random.rand(1, output_dim)\n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output\n",
    "\n",
    "input_dim = 2\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "weights_hidden, bias_hidden, weights_output, bias_output = initialize_parameters(input_dim, hidden_dim, output_dim)\n",
    "weights_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU neuron\"\"\"\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer neuron outputs\n",
    "# h_n1 = inputs[0]*weights[0][0] + inputs[0]*weights[0][1] + bias[0]\n",
    "# h_n2 = inputs[1]*weights[1][0] + inputs[1]*weights[1][1] + bias[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hidden layer neuron activation funcs.\n",
    "# h_o1 = sigmoid(h_n1)\n",
    "# h_o2 = sigmoid(h_n2)\n",
    "\n",
    "# # bias on output neurons\n",
    "# o_bias = 0.2\n",
    "\n",
    "# # weights taken by hidden layer's neurons to output layer\n",
    "# o_weight = [0.7,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.992233851517446\n",
      "Final output after activation func. :  0.992233851517446\n"
     ]
    }
   ],
   "source": [
    "# output = h_o1*o_weight[0] + h_o2*o_weight[1] + o_bias\n",
    "# print(\"Output:\",output)\n",
    "# print('Final output after activation func. : ',relu(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    hidden_activation = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "    output = sigmoid(np.dot(hidden_activation, weights_output) + bias_output)\n",
    "    return hidden_activation, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def backward_propagation(X, y, hidden_activation, output, weights_output):\n",
    "    output_error = 2 * (output - y) * sigmoid_derivative(output)\n",
    "    d_weights_output = np.dot(hidden_activation.T, output_error)\n",
    "    d_bias_output = np.sum(output_error, axis=0, keepdims=True)\n",
    "\n",
    "    hidden_error = np.dot(output_error, weights_output.T) * sigmoid_derivative(hidden_activation)\n",
    "    d_weights_hidden = np.dot(X.reshape(1, -1).T, hidden_error)  # Reshape X for proper dot product\n",
    "    d_bias_hidden = hidden_error\n",
    "\n",
    "    return d_weights_hidden, d_bias_hidden, d_weights_output, d_bias_output\n",
    "\n",
    "def update_weights(weights_hidden, bias_hidden, weights_output, bias_output, d_weights_hidden, d_bias_hidden, d_weights_output, d_bias_output, learning_rate):\n",
    "    weights_hidden -= learning_rate * d_weights_hidden\n",
    "    bias_hidden -= learning_rate * d_bias_hidden\n",
    "    weights_output -= learning_rate * d_weights_output\n",
    "    bias_output -= learning_rate * d_bias_output\n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final outputs for XOR dataset:\n",
      "Input: [0 0], Predicted Output: [[0.51536608]]\n",
      "Input: [0 1], Predicted Output: [[0.50155733]]\n",
      "Input: [1 0], Predicted Output: [[0.50724056]]\n",
      "Input: [1 1], Predicted Output: [[0.49874157]]\n"
     ]
    }
   ],
   "source": [
    "# XOR problem dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X)):\n",
    "        # Forward propagation\n",
    "        hidden_activation, output = forward_propagation(X[i], weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = mean_squared_error(y[i], output)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward propagation\n",
    "        d_weights_hidden, d_bias_hidden, d_weights_output, d_bias_output = backward_propagation(X[i], y[i], hidden_activation, output, weights_output)\n",
    "\n",
    "        # Update weights\n",
    "        weights_hidden, bias_hidden, weights_output, bias_output = update_weights(weights_hidden, bias_hidden, weights_output, bias_output, d_weights_hidden, d_bias_hidden, d_weights_output, d_bias_output, learning_rate)\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        avg_loss = total_loss / len(X)\n",
    "        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Final output for XOR dataset\n",
    "print(\"Final outputs for XOR dataset:\")\n",
    "for i in range(len(X)):\n",
    "    _, output = forward_propagation(X[i], weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weights of each neuron\n",
    "def print_weights(weights_hidden, weights_output):\n",
    "    print(\"Weights (Hidden Layer):\")\n",
    "    print(weights_hidden)\n",
    "    print(\"Weights (Output Layer):\")\n",
    "    print(weights_output)\n",
    "\n",
    "# Print weights every 10 epochs\n",
    "if (epoch + 1) % 10 == 0:\n",
    "    print_weights(weights_hidden, weights_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training:\n",
      "Weights (Hidden Layer):\n",
      "[[0.86784222 0.63028075]\n",
      " [0.84563358 0.01209098]]\n",
      "Weights (Output Layer):\n",
      "[[-0.32186108]\n",
      " [ 0.19239309]]\n"
     ]
    }
   ],
   "source": [
    "# Print weights of each neuron\n",
    "def print_weights(weights_hidden, weights_output):\n",
    "    print(\"Weights (Hidden Layer):\")\n",
    "    print(weights_hidden)\n",
    "    print(\"Weights (Output Layer):\")\n",
    "    print(weights_output)\n",
    "\n",
    "# Print weights after training\n",
    "print(\"Weights after training:\")\n",
    "print_weights(weights_hidden, weights_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
